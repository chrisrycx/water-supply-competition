{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SNODAS data\n",
    "Download all SNODAS SWE data for training and testing. Convert to total SWE by basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data functions from driven data but updated to return in-memory file\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import io\n",
    "import requests\n",
    "\n",
    "def date_url(date: datetime) -> str:\n",
    "    \"\"\"Get the source url for the SNODAS file on a given date\"\"\"\n",
    "    url = \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked/\"\n",
    "    url += f\"{date.year}/{date.month:02}_{calendar.month_abbr[date.month]}\"\n",
    "    url += f\"/SNODAS_{date.strftime('%Y%m%d')}.tar\"\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def download_from_url(source_url: str) -> io.BytesIO:\n",
    "    \"\"\"\n",
    "    Download a SNODAS file based on its source URL. \n",
    "    Returns the data TAR file as a BytesIO object.\n",
    "    \"\"\"\n",
    "    response = requests.get(source_url)\n",
    "    datatar = io.BytesIO()\n",
    "    datatar.write(response.content)\n",
    "    datatar.seek(0)\n",
    "\n",
    "    return datatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert downloaded data to GeoTIFF stored in temporary file\n",
    "\n",
    "import tarfile\n",
    "import gzip\n",
    "\n",
    "def saveTiffFromTar(snodas_tar, tiffname):\n",
    "    '''Converts a SNODAS tar file to a GeoTIFF and saves it to disk\n",
    "    \n",
    "    tarfile should be in memory as a BytesIO object\n",
    "    '''\n",
    "    # Open TAR and extract swe related files to in memory bytes\n",
    "    tar = tarfile.open(fileobj=snodas_tar, mode='r')\n",
    "    for tarinfo in tar:\n",
    "        if (tarinfo.name[8:12]=='1034') and (tarinfo.name[-6:]=='dat.gz'):\n",
    "            #print(f'Extracting: {tarinfo.name}')\n",
    "            gzfile_io = tar.extractfile(tarinfo)\n",
    "            swe_zipfile = gzip.GzipFile(fileobj=gzfile_io)\n",
    "\n",
    "        if (tarinfo.name[8:12]=='1034') and (tarinfo.name[-6:]=='txt.gz'):\n",
    "            #print(f'Extracting: {tarinfo.name}')\n",
    "            gzfile_io = tar.extractfile(tarinfo)\n",
    "            swehdr_zipfile = gzip.GzipFile(fileobj=gzfile_io, mode='r')\n",
    "\n",
    "    tar.close()\n",
    "\n",
    "    # Edit one line of header file so that it points to the data file\n",
    "    swehdr_lines = io.TextIOWrapper(swehdr_zipfile).readlines() #Wrapper needed to read file in text mode\n",
    "    \n",
    "    # Search for line that starts with 'Data file pathname:' and get the index\n",
    "    pathname_index = None\n",
    "    for i, line in enumerate(swehdr_lines):\n",
    "        if line.startswith('Data file pathname:'):\n",
    "            pathname_index = i\n",
    "            break\n",
    "\n",
    "    # Replace the line with the correct path\n",
    "    if pathname_index is not None:\n",
    "        swehdr_lines[pathname_index] = \"Data file pathname: swe.dat\\n\"\n",
    "    else:\n",
    "        print('Error: Data file pathname not found in header file')\n",
    "\n",
    "    with open('./data/snodas/temp/swe.dat', 'wb') as f:\n",
    "        f.write(swe_zipfile.read())\n",
    "\n",
    "    with open('./data/snodas/temp/swe.txt', 'w') as f:\n",
    "        f.writelines(swehdr_lines)\n",
    "\n",
    "    with rasterio.open('./data/snodas/temp/swe.txt') as src:\n",
    "        src_profile = src.profile\n",
    "        # Switch driver to tiff with compression\n",
    "        src_profile.update(driver = 'GTiff',compress='lzw',)\n",
    "\n",
    "        with rasterio.open(tiffname, 'w', **src_profile) as dst:\n",
    "            dst.write(src.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swedate = datetime(2017, 4, 8)\n",
    "sweurl = date_url(swedate)\n",
    "swe_tar = download_from_url(sweurl)\n",
    "saveTiffFromTar(swe_tar, './data/snodas/temp/swe.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('./data/snodas/temp/swe040117test.txt') as src:\n",
    "        src_profile = src.profile\n",
    "        # Switch driver to tiff with compression\n",
    "        src_profile.update(driver = 'GTiff',compress='lzw',)\n",
    "\n",
    "        with rasterio.open('./data/snodas/temp/swe.tiff', 'w', **src_profile) as dst:\n",
    "            dst.write(src.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate stats and compare with QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = zonal_stats('./data/geospatial.gpkg', './data/snodas/temp/swe.tif', stats=['sum'], geojson_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pecos, the sum matches the sum calculated in QGIS exactly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine dates for testing and training\n",
    "I should only need dates between January and April since each regression will be predicting 4/1 snowpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "month_days = [1,8,15,22]\n",
    "\n",
    "# Generate a datetime index with days of each month in month_days\n",
    "date_rng = pd.date_range(start='1/1/2005', end='1/1/2024', freq='D',inclusive='left')\n",
    "date_rng = date_rng[date_rng.day.isin(month_days)]\n",
    "\n",
    "# Filter values from outside the January thru July period\n",
    "date_rng = date_rng[(date_rng.month >= 1) & (date_rng.month <= 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data Loop\n",
    "Download and process each day of data in the range. Run and save each year separately in case there is a failure and a need to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: './data/snodas/temp/swe.txt' not recognized as a supported file format.\n",
      "Skipping date 2017-04-01 00:00:00\n",
      "Processing: 2023-04-22 00:00:00\r"
     ]
    }
   ],
   "source": [
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "for year in range(start_year, 2024):\n",
    "    date_rng_year = date_rng[date_rng.year == year]\n",
    "\n",
    "    # Initialize list to store data\n",
    "    data = []\n",
    "    processed_dates = [] # Keep track of dates that have been processed in case of error\n",
    "\n",
    "    # Download and process data for dates in date_rng_2005\n",
    "    for swe_date in date_rng_year:\n",
    "        print(f'Processing: {swe_date}\\r', end='')\n",
    "        sweurl = date_url(swe_date)\n",
    "        swe_tar = download_from_url(sweurl)\n",
    "        try:\n",
    "            saveTiffFromTar(swe_tar, './data/snodas/temp/swe.tif')\n",
    "        except RasterioIOError as rioe:\n",
    "            print(f'Error: {rioe}')\n",
    "            print (f'Skipping date {swe_date}')\n",
    "            continue\n",
    "        stats = zonal_stats('./data/geospatial.gpkg', './data/snodas/temp/swe.tif', stats=['sum'], geojson_out=True)\n",
    "        processed_dates.append(swe_date)\n",
    "        \n",
    "        basin_swe = {}\n",
    "        for basin in stats:\n",
    "            basin_swe[basin['properties']['name']] = basin['properties']['sum']\n",
    "\n",
    "        data.append(basin_swe)\n",
    "\n",
    "    # Create dataframe from data list\n",
    "    df = pd.DataFrame(data, index=pd.DatetimeIndex(processed_dates))\n",
    "\n",
    "    # Save dataframe to CSV\n",
    "    df.to_csv(f'./data/snodas/snodas_swe_{year}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-01-01', '2017-01-08', '2017-01-15', '2017-01-22',\n",
       "               '2017-02-01', '2017-02-08', '2017-02-15', '2017-02-22',\n",
       "               '2017-03-01', '2017-03-08', '2017-03-15', '2017-03-22',\n",
       "               '2017-04-01', '2017-04-08', '2017-04-15', '2017-04-22'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_rng_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".drivendata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
